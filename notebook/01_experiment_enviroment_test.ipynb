{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc0ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828c1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from xopen import xopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fcaa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lost_in_the_middle.prompting import (\n",
    "    Document,\n",
    "    get_closedbook_qa_prompt,\n",
    "    get_qa_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbcc90c",
   "metadata": {},
   "source": [
    "# get_qa_response_from_mpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e23583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    input_path,\n",
    "    model_name,\n",
    "    temperature,\n",
    "    top_p,\n",
    "    batch_size,\n",
    "    closedbook,\n",
    "    prompt_mention_random_ordering,\n",
    "    use_random_ordering,\n",
    "    query_aware_contextualization,\n",
    "    num_gpus,\n",
    "    max_memory_per_gpu,\n",
    "    max_new_tokens,\n",
    "    output_path,\n",
    "):\n",
    "    # Create directory for output path if it doesn't exist.\n",
    "    pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    examples = []\n",
    "    prompts = []\n",
    "    all_model_documents = []\n",
    "    did_format_warn = False\n",
    "\n",
    "    # Fetch all of the prompts\n",
    "    with xopen(input_path) as fin:\n",
    "        for line in tqdm(fin):\n",
    "            input_example = json.loads(line)\n",
    "            # Get the prediction for the input example\n",
    "            question = input_example[\"question\"]\n",
    "            if closedbook:\n",
    "                documents = []\n",
    "            else:\n",
    "                documents = []\n",
    "                for ctx in deepcopy(input_example[\"ctxs\"]):\n",
    "                    documents.append(Document.from_dict(ctx))\n",
    "                if not documents:\n",
    "                    raise ValueError(f\"Did not find any documents for example: {input_example}\")\n",
    "            \n",
    "            # Put some of the ordering here and put it in functions\n",
    "            if use_random_ordering:\n",
    "                # Randomly order only the distractors (isgold is False), keeping isgold documents\n",
    "                # at their existing index.\n",
    "                (original_gold_index,) = [idx for idx, doc in enumerate(documents) if doc.isgold is True]\n",
    "                original_gold_document = documents[original_gold_index]\n",
    "                distractors = [doc for doc in documents if doc.isgold is False]\n",
    "                random.shuffle(distractors)\n",
    "                distractors.insert(original_gold_index, original_gold_document)\n",
    "                documents = distractors\n",
    "\n",
    "            if closedbook:\n",
    "                prompt = get_closedbook_qa_prompt(question)\n",
    "            else:\n",
    "                prompt = get_qa_prompt(\n",
    "                    question,\n",
    "                    documents,\n",
    "                    mention_random_ordering=prompt_mention_random_ordering,\n",
    "                    query_aware_contextualization=query_aware_contextualization,\n",
    "                )\n",
    "\n",
    "            if \"instruct\" in model_name:\n",
    "                if did_format_warn is False:\n",
    "                    did_format_warn = True\n",
    "                prompt = format_instruct_prompt(prompt)\n",
    "            prompts.append(prompt)\n",
    "            examples.append(deepcopy(input_example))\n",
    "            all_model_documents.append(documents)\n",
    "\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    config.attn_config[\"attn_impl\"] = \"triton\"\n",
    "    config.max_seq_len = 16384\n",
    "    # (input + output) tokens can now be up to 16384\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # to avoid an error\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for batched_prompts in tqdm(chunks(prompts, batch_size), total=math.ceil(len(prompts) / batch_size)):\n",
    "        inputs = tokenizer(batched_prompts, return_tensors=\"pt\", padding=True)\n",
    "        outputs = [x[:100].split(\" \") for x in batched_prompts]\n",
    "        responses.append(outputs)\n",
    "\n",
    "    with xopen(output_path, \"w\") as f:\n",
    "        for example, model_documents, prompt, response in zip(examples, all_model_documents, prompts, responses):\n",
    "            output_example = deepcopy(example)\n",
    "            # Add some extra metadata to the output example\n",
    "            output_example[\"model_prompt\"] = prompt\n",
    "            output_example[\"model_documents\"] = [dataclasses.asdict(document) for document in model_documents]\n",
    "            output_example[\"model_answer\"] = output_example[\"answers\"]\n",
    "            #output_example[\"model_answer\"] = response\n",
    "            output_example[\"model\"] = model_name\n",
    "            output_example[\"model_temperature\"] = temperature\n",
    "            output_example[\"model_top_p\"] = top_p\n",
    "            output_example[\"model_prompt_mention_random_ordering\"] = prompt_mention_random_ordering\n",
    "            output_example[\"model_use_random_ordering\"] = use_random_ordering\n",
    "            f.write(json.dumps(output_example) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd383304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ef7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruct_prompt(instruction):\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    INTRO_BLURB = (\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    )\n",
    "    PROMPT_FOR_GENERATION = \"{intro}\\n{instruction_key}\\n{instruction}\\n{response_key}\\n\".format(\n",
    "        intro=INTRO_BLURB,\n",
    "        instruction_key=INSTRUCTION_KEY,\n",
    "        instruction=instruction,\n",
    "        response_key=RESPONSE_KEY,\n",
    "    )\n",
    "    return PROMPT_FOR_GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d0896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2655it [00:00, 4263.09it/s]\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "100%|█████████████████████████████████████| 2655/2655 [00:02<00:00, 1312.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "        \"../qa_data/nq-open-oracle.jsonl.gz\",\n",
    "        \"mosaicml/mpt-30b-instruct\",\n",
    "        0.0,\n",
    "        1,\n",
    "        1,\n",
    "        False,\n",
    "        False,\n",
    "        False,\n",
    "        True,\n",
    "        1,\n",
    "        40,\n",
    "        100,\n",
    "        \"../qa_predictions/nq-open-oracle-mpt-30b-instruct-predictions.jsonl.gz\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1ad58",
   "metadata": {},
   "source": [
    "# evaluate_qa_responses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ebec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "from lost_in_the_middle.metrics import best_subspan_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b9b3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    (best_subspan_em, \"best_subspan_em\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01fad660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<function lost_in_the_middle.metrics.best_subspan_em(prediction: str, ground_truths: List[str]) -> float>,\n",
       "  'best_subspan_em')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f373d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    input_path,\n",
    "    output_path,\n",
    "):\n",
    "    all_examples = []\n",
    "    with xopen(input_path) as fin:\n",
    "        for line in tqdm(fin):\n",
    "            input_example = json.loads(line)\n",
    "            all_examples.append(input_example)\n",
    "\n",
    "    # Compute normal metrics in parallel, if applicable\n",
    "    all_example_metrics = []\n",
    "    for example in tqdm(all_examples):\n",
    "        all_example_metrics.append(get_metrics_for_example(example))\n",
    "\n",
    "    # Average metrics across examples\n",
    "    for (_, metric_name) in METRICS:\n",
    "        average_metric_value = statistics.mean(\n",
    "            example_metrics[metric_name] for (example_metrics, _) in all_example_metrics\n",
    "        )\n",
    "        print(f\"{metric_name}: {average_metric_value}\")\n",
    "\n",
    "    if output_path:\n",
    "        with xopen(output_path, \"w\") as f:\n",
    "            for (example_metrics, example) in all_example_metrics:\n",
    "                example_with_metrics = deepcopy(example)\n",
    "                for metric_name, metric_value in example_metrics.items():\n",
    "                    example_with_metrics[f\"metric_{metric_name}\"] = metric_value\n",
    "                f.write(json.dumps(example_with_metrics) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b5084d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_for_example(example):\n",
    "    gold_answers = example[\"answers\"]\n",
    "    model_answer = example[\"model_answer\"]\n",
    "\n",
    "    # NOTE: we take everything up to the first newline, since otherwise models could hack\n",
    "    # the metric by simply copying te input context (as the gold answer is guaranteed\n",
    "    # to occur in the input context).\n",
    "    model_answer = \"; \".join(model_answer).split(\"\\n\")[0].strip()\n",
    "    \n",
    "    example_metrics = {}\n",
    "    for (metric, metric_name) in METRICS:\n",
    "        example_metrics[metric_name] = metric(prediction=model_answer, ground_truths=gold_answers)\n",
    "    return (example_metrics, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9659bc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2655it [00:00, 29126.20it/s]\n",
      "100%|████████████████████████████████████| 2655/2655 [00:00<00:00, 29051.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_subspan_em: 1.0\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "        \"../qa_predictions/nq-open-oracle-mpt-30b-instruct-predictions.jsonl.gz\",\n",
    "        \"../qa_predictions/nq-open-oracle-mpt-30b-instruct-predictions-scored.jsonl.gz\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c5d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227c1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c668ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
